metadata:
  id: riksdagens-korpus
  language: swe
  name:
    eng: Riksdagens korpus

# classes:
#   token:msd: <token>:stanza.msd

# stanza:
#     dep_model: stanza/dep/sv_talbanken_parser.pt
#     lem_model: stanza/lem/sv_suc_lemmatizer.pt
#     pos_model: stanza/pos/full_sv_talbanken_tagger.pt
#     pretrain_dep_model: stanza/pos/full_sv_talbanken.pretrain.pt
#     pretrain_pos_model: stanza/pos/full_sv_talbanken.pretrain.pt
#     resources_file: stanza/resources.json

import:
  # source_dir          │ Location of input documents, defaults to source, recursive.
  # importer            │ Specifies importer to use,xml_import:parse (default) or text_import:parse
  # document_annotation │ Specifies the annotation representing one text document.
  # encoding            | Specifies encoding of source documents. It defaults to UTF-8.
  # normalize           | Specifies unicode normalization (‘NFC’ (default), ‘NFKC’, ‘NFD’, or ‘NFKD’).
  # keep_control_chars  | Disables control characters removal.
  importer: xml_import:parse
  source_dir: source
  document_annotation: speech


xml_import:
  #  elements            │  List of elements and attributes in source document.
  #  encoding            │  Encoding of source document. Defaults to UTF-8.
  #  header_data         │  List of header elements and attributes from which to extract metadata.
  #  header_elements     │  Elements containing header metadata. Contents will not be included in corpus text.
  #  keep_control_chars  │  Set to True if control characters should not be removed from the text.
  #  normalize           │  Normalize input using any of the following forms: 'NFC', 'NFKC', 'NFD', and 'NFKD'.
  #  prefix              │  Optional prefix to add to annotation names.
  #  skip                │  Elements and attributes to skip. Use elementname:@contents to skip contents as well.
  skip:
  - protocol:xmlns

# text_import:
  # encoding            │  Encoding of source document. Defaults to UTF-8.
  # keep_control_chars  │  Set to True if control characters should not be removed from the text.
  # normalize           │  Normalize input using any of the following forms: 'NFC', 'NFKC', 'NFD', and 'NFKD'.
  # prefix              │  Optional prefix to add to annotation names.

export:
  annotations:
  - <token>:saldo.baseform
  - <token>:stanza.msd
  - <token>:stanza.pos
  default: # `sparv run` defaults
      # - xml_export:combined
      - xml_export:pretty
      - csv_export:csv
      - stats_export:freq_list_simple
  remove_module_namespaces: true
  scramble_on: <sentence>
  word: <token:word>

csv_export:
  # csv_export.annotations           │  Sparv annotations to include.
  # csv_export.delimiter             │  Delimiter separating fields.
  # csv_export.source_annotations    │  List of annotations and attributes from the source data to include.
  # export.remove_module_namespaces  │  Remove module name prefixes from annotation names in export
  # export.source_namespace          │  Prefix to add to the names of all annotations from source
  # export.sparv_namespace           │  Prefix to add to the names of all automatically created annotations
  # export.word                      │  Annotation to use as token text in export
  annotations:
      - <token>:saldo.baseform
      - <token>:stanza.msd
      - <token>:stanza.pos
  delimiter: "\t"

# stats_export:
#   freq_list_simple:
#     # metadata.id                         │  Machine name of corpus (a-z, 0-9, -)
#     # stats_export.cutoff                 │  The minimum frequency a word must have in order to be included in
#     #                                     │  the result
#     # stats_export.delimiter              │  Delimiter separating columns
#     # stats_export.include_all_compounds  │  Whether to include compound analyses for every word or just
#     #                                     │  for the words that are lacking a sense annotation
#   freq_list:
#     # metadata.id                         │  Machine name of corpus (a-z, 0-9, -)
#     # stats_export.cutoff                 │  The minimum frequency a word must have in order to be included in
#     #                                     │  the result
#     # stats_export.delimiter              │  Delimiter separating columns

segment:
    # paragraph_chunk: <text>              │  Chunk to use for automatic sentence segmentation (typically <text> or nothing)
    # paragraph_segmenter: blanklines      │  How to do automatic paragraph segmentation. Valid values: blanklines, linebreaks, whitespace, fsv_paragraph
    # sentence_chunk: <paragraph>, <text>  │  Chunk to use for automatic sentence segmentation (typically <text> or <paragraph>)
    # token_chunk: <sentence>              │  Chunk to use for automatic tokenisation
    paragraph_chunk: <speech>
    paragraph_segmenter: blanklines
    sentence_chunk: <speech>
